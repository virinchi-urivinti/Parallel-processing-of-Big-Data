{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ro1i1KWX87e"
   },
   "outputs": [],
   "source": [
    "import warc\n",
    "import urllib2\n",
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "import io\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8KqSYNMYE_Y"
   },
   "outputs": [],
   "source": [
    "def search_domain(domain):\n",
    "\n",
    "  record_list = []\n",
    "  print \"[*] Trying target domain: %s\" % domain\n",
    "  \n",
    "  for index in index_list:\n",
    "      \n",
    "      #print \"[*] Trying index %s\" % index\n",
    "      \n",
    "      cc_url  = \"http://index.commoncrawl.org/CC-MAIN-%s-index?\" % index\n",
    "      cc_url += \"url=%s&matchType=domain&output=json\" % domain\n",
    "      \n",
    "      response = requests.get(cc_url)\n",
    "      \n",
    "      if response.status_code == 200:\n",
    "          \n",
    "          records = response.content.splitlines()\n",
    "          \n",
    "          for record in records:\n",
    "              record_list.append(json.loads(record))\n",
    "\n",
    "  return record_list        \n",
    "\n",
    "\n",
    "def index_url(site,keyword):\n",
    "  index_name = 'https://index.commoncrawl.org/CC-MAIN-2019-13-index?url='+site+'%2F'+keyword+'&output=json' # star symbol ivalli\n",
    "  resp = requests.get(index_name)\n",
    "  #print(resp)\n",
    "  return resp\n",
    "\n",
    "def pages(response):\n",
    "  try:\n",
    "    from cStringIO import StringIO\n",
    "  except:\n",
    "    from StringIO import StringIO\n",
    "  pages = [json.loads(x) for x in response.content.strip().split('\\n')]\n",
    "  #print('length of pages:',len(pages))\n",
    "  data_whole =[]\n",
    "  data_http  =[]\n",
    "  for count in range(0,len(pages)):\n",
    "    if count <= 50 :\n",
    "      page =pages[count]\n",
    "      offset, length = int(page['offset']), int(page['length'])\n",
    "      offset_end = offset + length - 1\n",
    "      prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "      resp = requests.get(prefix + page['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "      raw_data = StringIO(resp.content)\n",
    "      f = gzip.GzipFile(fileobj=raw_data)\n",
    "      data = f.read()\n",
    "      dd,links,http_link = beautifulsoup_ccw(data)\n",
    "      data_whole.append(np.unique(links))\n",
    "      data_http.append(http_link)\n",
    "    else:\n",
    "      break;\n",
    "  return data,dd,links,data_whole,data_http\n",
    "\n",
    "def beautifulsoup_ccw(data):\n",
    "  links =[]\n",
    "  http_links =[]\n",
    "  soup = BeautifulSoup(data)\n",
    "  dd = soup.find_all('a')\n",
    "  for link in soup.find_all('a'):#,class_ = 'realStory'):\n",
    "    if link.has_attr('href'): # print(link)\n",
    "      if('http' in link['href']):\n",
    "        #print('')\n",
    "        http_links.append(link['href']) \n",
    "      else:\n",
    "        if link['href'] not in link :\n",
    "          sub_index = link['href'].find('story')\n",
    "          if sub_index != -1:\n",
    "            print(link['href'])\n",
    "            links.append(link['href'])\n",
    "          else:\n",
    "            pass\n",
    "        else:\n",
    "          pass\n",
    "    else:\n",
    "      break;\n",
    "  return dd,links,http_links\n",
    "\n",
    "def beautifulsoup_urlhit(prefix,all_links)\n",
    "  from tqdm import tqdm\n",
    "  paragraph =[]\n",
    "  for count in range(0,len(all_links)):\n",
    "    for i in all_links[count]:\n",
    "      link = i\n",
    "      quote_page = prefix+link\n",
    "      from urllib2 import Request, urlopen, URLError\n",
    "      req =urllib2.Request(quote_page)\n",
    "      try:\n",
    "        response = urlopen(req)\n",
    "      except URLError as e:\n",
    "        if hasattr(e, 'reason'):\n",
    "          print 'We failed to reach a server.'\n",
    "          print 'Reason: ', e.reason\n",
    "        elif hasattr(e, 'code'):\n",
    "          print 'The server couldn\\'t fulfill the request.'\n",
    "          print 'Error code: ', e.code\n",
    "      else:\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        find = soup.find_all('p')\n",
    "        for text in find:\n",
    "          f= open(\"commoncrawl_soccer.txt\",\"a+\")\n",
    "          f.write(text.get_text().encode('utf-8')) \n",
    "          f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRDSoWJRPwGx"
   },
   "outputs": [],
   "source": [
    "index_list= [\"2019-13\",\"2019-09\",\"2019-04\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZiQ95_IYnnu"
   },
   "outputs": [],
   "source": [
    "resp_s = index_url('espn.com','soccer*')\n",
    "data_s,bs_s,links_s,all_links,http_link = pages(resp_s)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zC8xShKXhjtu"
   },
   "outputs": [],
   "source": [
    "prefix = 'http://www.espn.com'\n",
    "urls = all_links\n",
    "beautifulsoup_urlhit(prefix,urls)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cc_final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
