{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nytimesarticle import articleAPI\n",
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = articleAPI('4cEeTFXUHuWuNsTopsd0wGQshokOlA2h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=[]\n",
    "for i in range(0,15):\n",
    "    articles.append(api.search(q = 'basketball',page=i,begin_date=20190219))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls_basket=[]\n",
    "#for article in articles:\n",
    "#    for key in article['response']['docs']:\n",
    "#        urls_basket.append(key['web_url'])\n",
    "len(urls_basket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_foot=[]\n",
    "for i in range(0,20):\n",
    "    articles_foot.append(api.search(q = 'football',page=i,begin_date=20190219))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls_foot=[]\n",
    "#for article1 in articles_foot:\n",
    "#    for key in article1['response']['docs']:\n",
    " #       urls_foot.append(key['web_url'])\n",
    "len(urls_foot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_golf=[]\n",
    "for i in range(0,20):\n",
    "    articles_golf.append(api.search(q = 'golf',page=i,begin_date=20190219))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls_golf=[]\n",
    "#for article2 in articles_golf:\n",
    "#    for key in article2['response']['docs']:\n",
    "#        urls_golf.append(key['web_url'])\n",
    "len(urls_golf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_soccer=[]\n",
    "for i in range(0,20):\n",
    "    articles_soccer.append(api.search(q = 'soccer',page=i,begin_date=20190219))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_soccer=[]\n",
    "for article3 in articles_soccer:\n",
    "    for key in article3['response']['docs']:\n",
    "        urls_soccer.append(key['web_url'])\n",
    "len(urls_soccer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_baseball=[]\n",
    "for i in range(0,20):\n",
    "    articles_baseball.append(api.search(q = 'baseball',page=i,begin_date=20190219))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls_baseball=[]\n",
    "#for article4 in articles_baseball:\n",
    "#    for key in article4['response']['docs']:\n",
    " #       urls_baseball.append(key['web_url'])\n",
    "len(urls_baseball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_tennis=[]\n",
    "for i in range(0,10):\n",
    "    articles_tennis.append(api.search(q = 'tennis',page=i,begin_date=20190219))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_tennis=[]\n",
    "for article5 in articles_tennis:\n",
    "    for key in article5['response']['docs']:\n",
    "        urls_tennis.append(key['web_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(urls_basket))\n",
    "print(len(urls_foot))\n",
    "print(len(urls_golf))\n",
    "print(len(urls_baseball))\n",
    "print(len(urls_soccer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "for i in urls_basket:\n",
    "    urls.append(i)\n",
    "print(len(urls))\n",
    "for j in urls_baseball:\n",
    "    urls.append(j)\n",
    "print(len(urls))\n",
    "for k in urls_foot:\n",
    "    urls.append(k)\n",
    "print(len(urls))\n",
    "for l in urls_golf:\n",
    "    urls.append(l)\n",
    "print(len(urls))\n",
    "for m in urls_soccer:\n",
    "    urls.append(m)\n",
    "print(len(urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nytimes_basket_n.txt\", \"w\",encoding='utf-8') as file1:\n",
    "    content=\"\"\n",
    "    for url in urls_basket:\n",
    "        htmlData1=[]\n",
    "        htmlData1=requests.get(url)\n",
    "        if(htmlData1.status_code==200):\n",
    "        #print(\"hello\")\n",
    "            soup=BeautifulSoup(htmlData1.content,'html.parser')\n",
    "            results1 = soup.findAll('p')\n",
    "        for para in results1:\n",
    "            file1.write(str((para.get_text())))\n",
    "            \n",
    "            file1.write(\"\\n\")\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nytimes_baseball_n.txt\", \"w\",encoding='utf-8') as file2:\n",
    "    content=\"\"\n",
    "    for url in urls_baseball:\n",
    "        htmlData2=[]\n",
    "        htmlData2=requests.get(url)\n",
    "        if(htmlData2.status_code==200):\n",
    "        #print(\"hello\")\n",
    "            soup=BeautifulSoup(htmlData2.content,'html.parser')\n",
    "            results2 = soup.findAll('p')\n",
    "        for para in results2:\n",
    "            file2.write(str((para.get_text())))\n",
    "            \n",
    "            file2.write(\"\\n\")\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nytimes_foot_n.txt\", \"w\",encoding='utf-8') as file3:\n",
    "    content=\"\"\n",
    "    for url in urls_foot:\n",
    "        htmlData3=[]\n",
    "        htmlData3=requests.get(url)\n",
    "        if(htmlData3.status_code==200):\n",
    "        #print(\"hello\")\n",
    "            soup=BeautifulSoup(htmlData3.content,'html.parser')\n",
    "            results3 = soup.findAll('p')\n",
    "        for para in results3:\n",
    "            file3.write(str((para.get_text())))\n",
    "            \n",
    "            file3.write(\"\\n\")\n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nytimes_soccer_n.txt\", \"w\",encoding='utf-8') as file4:\n",
    "    content=\"\"\n",
    "    for url in urls_soccer:\n",
    "        htmlData4=[]\n",
    "        htmlData4=requests.get(url)\n",
    "        if(htmlData4.status_code==200):\n",
    "        #print(\"hello\")\n",
    "            soup=BeautifulSoup(htmlData4.content,'html.parser')\n",
    "            results4 = soup.findAll('p')\n",
    "        for para in results4:\n",
    "            file4.write(str((para.get_text())))\n",
    "            \n",
    "            file4.write(\"\\n\")\n",
    "file4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nytimes_golf_n.txt\", \"w\",encoding='utf-8') as file5:\n",
    "    content=\"\"\n",
    "    for url in urls_golf:\n",
    "        htmlData5=[]\n",
    "        htmlData5=requests.get(url)\n",
    "        if(htmlData5.status_code==200):\n",
    "        #print(\"hello\")\n",
    "            soup=BeautifulSoup(htmlData5.content,'html.parser')\n",
    "            results5 = soup.findAll('p')\n",
    "        for para in results5:\n",
    "            file5.write(str((para.get_text())))\n",
    "            \n",
    "            file5.write(\"\\n\")\n",
    "file5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nytimes_tennis.txt\", \"w\",encoding='utf-8') as file6:\n",
    "    content=\"\"\n",
    "    for url in urls_tennis:\n",
    "        htmlData6=[]\n",
    "        htmlData6=requests.get(url)\n",
    "        if(htmlData6.status_code==200):\n",
    "        #print(\"hello\")\n",
    "            soup=BeautifulSoup(htmlData6.content,'html.parser')\n",
    "            results6 = soup.findAll('p')\n",
    "        for para in results6:\n",
    "            file6.write(str((para.get_text())))\n",
    "            \n",
    "            file6.write(\"\\n\")\n",
    "file6.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
